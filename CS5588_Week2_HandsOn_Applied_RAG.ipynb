{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ango3636/CS5588DSCapstone/blob/update-notebook/CS5588_Week2_HandsOn_Applied_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8beb036f",
      "metadata": {
        "id": "8beb036f"
      },
      "source": [
        "# CS 5588 — Week 2 Hands-On: Applied RAG for Product & Venture Development (Two-Step)\n",
        "**Initiation (20 min, Jan 27)** → **Completion (60 min, Jan 29)**\n",
        "\n",
        "**Submission:** Survey + GitHub  \n",
        "**Due:** **Jan 29 (Thu), end of class**\n",
        "\n",
        "## New Requirement (Important)\n",
        "For **full credit (2% individual)** you must:\n",
        "1) Use **your own project-aligned dataset** (not only benchmark)  \n",
        "2) Add **your own explanations** for key steps\n",
        "\n",
        "### ✅ “Cell Description” rule (same style as CS 5542)\n",
        "After each **IMPORTANT** code cell, add a short Markdown **Cell Description** (2–5 sentences):\n",
        "- What the cell does\n",
        "- Why it matters for a **product-grade** RAG system\n",
        "- Any design choices (chunk size, α, reranker, etc.)\n",
        "\n",
        "> Treat these descriptions as **mini system documentation** (engineering + product thinking).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e43e2d",
      "metadata": {
        "id": "d0e43e2d"
      },
      "source": [
        "## Project Dataset Guide (Required for Full Credit)\n",
        "\n",
        "### Minimum requirements\n",
        "- **5–25 documents** (start small; scale later)\n",
        "- Prefer **plain text** documents (`.txt`)\n",
        "- Put files in a folder named: `project_data/`\n",
        "\n",
        "### Recommended dataset types (choose one)\n",
        "- Policies / guidelines / compliance docs\n",
        "- Technical docs / manuals / SOPs\n",
        "- Customer support FAQs / tickets (de-identified)\n",
        "- Research notes / literature summaries\n",
        "- Domain corpus (healthcare, cybersecurity, business, etc.)\n",
        "\n",
        "> Benchmarks are optional, but **cannot** earn full credit by themselves.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f68d33",
      "metadata": {
        "id": "e7f68d33"
      },
      "source": [
        "## 0) One-Click Setup + Import Check  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "If you are in **Google Colab**, run the install cell below, then **Runtime → Restart session** if imports fail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ddaa1c18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddaa1c18",
        "outputId": "bf346449-ffcf-495d-8a49-24704770505e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "✅ If imports fail later: Runtime → Restart session and run again.\n"
          ]
        }
      ],
      "source": [
        "# CS 5588 Lab 2 — One-click dependency install (Colab)\n",
        "!pip -q install -U sentence-transformers chromadb faiss-cpu scikit-learn rank-bm25 transformers accelerate\n",
        "\n",
        "import sys, platform\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"✅ If imports fail later: Runtime → Restart session and run again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab532915",
      "metadata": {
        "id": "ab532915"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Write 2–5 sentences explaining what the setup cell does and why restarting the runtime sometimes matters after pip installs.\n",
        "\n",
        "This setup cell installs and updates all required Python libraries for the lab, including tools for embeddings, vector search, and retrieval models. It then prints the Python and platform information to help verify the runtime environment. Restarting the runtime after pip install is sometimes necessary because newly installed or upgraded packages may not be fully available to the current Python session until it reloads. Restarting ensures that imports use the updated dependencies instead of cached versions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49154e13",
      "metadata": {
        "id": "49154e13"
      },
      "source": [
        "# STEP 1 — INITIATION (Jan 27, 20 minutes)\n",
        "**Goal:** Define the **product**, **users**, **dataset reality**, and **trust risks**.\n",
        "\n",
        "> This is a **product milestone**, not a coding demo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58216603",
      "metadata": {
        "id": "58216603"
      },
      "source": [
        "## 1A) Product Framing (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Fill in the template below like a founder/product lead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "214ee1ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "214ee1ba",
        "outputId": "cad09d05-a787-4ed3-d255-b9fa1e0c2f16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product_name': 'Salesforce Assistant',\n",
              " 'target_users': 'Support Analysts',\n",
              " 'core_problem': \" The 'Execution Gap' between understanding a user and actually solving their problem.\",\n",
              " 'why_rag_not_chatbot': 'Guidelines are stored in a vector database for complex step by step related procedures as well as the ability to provide citations.',\n",
              " 'failure_harms_who_and_how': 'The end user loses trust, the support analyst and the organization loses credibility and is possibly financial discredited.'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "product = {\n",
        "  \"product_name\": \"Salesforce Assistant\",\n",
        "  \"target_users\": \"Support Analysts\",\n",
        "  \"core_problem\": \" The 'Execution Gap' between understanding a user and actually solving their problem.\",\n",
        "  \"why_rag_not_chatbot\": \"Guidelines are stored in a vector database for complex step by step related procedures as well as the ability to provide citations.\",\n",
        "  \"failure_harms_who_and_how\": \"The end user loses trust, the support analyst and the organization loses credibility and is possibly financial discredited.\",\n",
        "}\n",
        "product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "490a084a",
      "metadata": {
        "id": "490a084a"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain your product in 3–5 sentences: who the user is, what pain point exists today, and why grounded RAG helps.\n",
        "\n",
        "Our product is designed for customer support teams and compliance analysts who must navigate complex, rapidly changing internal regulations. Today, these professionals face \"information overload,\" where the gap between a static manual and a live customer conversation leads to inconsistent advice, policy violations, and high burnout. By using grounded RAG, our system ensures every AI-generated response is anchored to your latest verified SOPs, providing a transparent \"audit trail\" that eliminates hallucinations and guarantees that support actions remain both accurate and compliant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "179e8e12",
      "metadata": {
        "id": "179e8e12"
      },
      "source": [
        "## 1B) Dataset Reality Plan (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Describe where your data comes from **in the real world**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "282cb6f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "282cb6f9",
        "outputId": "363f80dc-fc7a-4ade-e513-2e9d38fb9c97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_owner': 'Corporate Legal, HR, and Technical Operations Teams',\n",
              " 'data_sensitivity': 'Regulated and Internal (Non-Public)',\n",
              " 'document_types': 'tandard Operating Procedures (SOPs), Product Technical Manuals, and internal Policy Memos',\n",
              " 'expected_scale_in_production': '5k to 50k documents (ranging from 1-page memos to 200-page regulatory filings)',\n",
              " 'data_reality_check_paragraph': \"In the real world, this data is 'messy'—it exists as scanned PDFs with complex tables, legacy Word docs with conflicting version histories, and internal Wikis that are partially outdated. Unlike a benchmark dataset, these documents often contain contradictory instructions where a new 'Policy Addendum' might override a section of a 'Master SOP' without the original being deleted. Our RAG system must handle this by prioritizing recent version metadata and resolving conflicts through hierarchical retrieval logic.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dataset_plan = {\n",
        "  \"data_owner\": \"Corporate Legal, HR, and Technical Operations Teams\",              # company / agency / public / internal team\n",
        "  \"data_sensitivity\": \"Regulated and Internal (Non-Public)\",        # public / internal / regulated / confidential\n",
        "  \"document_types\": \"tandard Operating Procedures (SOPs), Product Technical Manuals, and internal Policy Memos\",          # policies, manuals, reports, research, etc.\n",
        "  \"expected_scale_in_production\": \"5k to 50k documents (ranging from 1-page memos to 200-page regulatory filings)\",  # e.g., 200 docs, 10k docs, etc.\n",
        "  \"data_reality_check_paragraph\": \"In the real world, this data is 'messy'—it exists as scanned PDFs with complex tables, legacy Word docs with conflicting version histories, and internal Wikis that are partially outdated. Unlike a benchmark dataset, these documents often contain contradictory instructions where a new 'Policy Addendum' might override a section of a 'Master SOP' without the original being deleted. Our RAG system must handle this by prioritizing recent version metadata and resolving conflicts through hierarchical retrieval logic.\",\n",
        "}\n",
        "dataset_plan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e2da001",
      "metadata": {
        "id": "3e2da001"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Write 2–5 sentences describing where this data would come from in a real deployment and any privacy/regulatory constraints.\n",
        "\n",
        "An example of where this data could come from is Salesforce. As a support analyst, I utilize saleforce which contains old cases reported by users that contain comments and email conversation, photo attachments, case resolutions as well as knowledge based articles and known issues such as bugs, enhancements, or tasks. This type of data is private to the company and their solutions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df3ac72",
      "metadata": {
        "id": "2df3ac72"
      },
      "source": [
        "## 1C) User Stories + Mini Rubric (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Define **3 user stories** (U1 normal, U2 high-stakes, U3 ambiguous/failure) + rubric for evidence and correctness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0a72b8eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a72b8eb",
        "outputId": "2cff306a-c895-40f0-bd70-d4ca10b7739a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'U1_normal': {'user_story': 'As a Junior Support Agent, I want to quickly retrieve the specific return policy for a damaged item so that I can provide an accurate response without searching through 50+ PDFs.',\n",
              "  'acceptable_evidence': [\"A snippet from the 'Product Return SOP v2.1' dated within the last 12 months.\",\n",
              "   \"The specific 'Condition Grade' table mentioned in the technical manual.\"],\n",
              "  'correct_answer_must_include': [\"A direct quote from the policy regarding 'damaged on arrival' items.\",\n",
              "   'A clear citation link to the source document for agent verification.']},\n",
              " 'U2_high_stakes': {'user_story': \"As a Compliance Officer, I want the AI to verify that a customer is in a 'permitted jurisdiction' before the agent offers a software license refund so that we avoid violating international trade export laws.\",\n",
              "  'acceptable_evidence': [\"The current 'Global Export Control List' stored in the secure compliance folder.\",\n",
              "   \"The customer's verified account location metadata.\"],\n",
              "  'correct_answer_must_include': [\"A mandatory 'Stop/Proceed' check based on the retrieved legal guideline.\",\n",
              "   \"A warning if the customer's region is flagged as 'Restricted' or 'Sanctioned'.\"]},\n",
              " 'U3_ambiguous_failure': {'user_story': \"As a Senior Support Analyst, I want the system to flag a query as 'Unresolved' when the internal manuals contain conflicting instructions so that I can manually intervene before a mistake is made.\",\n",
              "  'acceptable_evidence': [\"Two or more retrieved chunks from different SOPs that provide contradictory steps (e.g., one says 'Refund' and another says 'Store Credit Only').\"],\n",
              "  'correct_answer_must_include': [\"An explicit admission of uncertainty: 'I found conflicting policies in Manual A and Manual B.'\",\n",
              "   'A request for human escalation rather than a synthesized (hallucinated) compromise.']}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "user_stories = {\n",
        "  \"U1_normal\": {\n",
        "    \"user_story\": \"As a Junior Support Agent, I want to quickly retrieve the specific return policy for a damaged item so that I can provide an accurate response without searching through 50+ PDFs.\",\n",
        "    \"acceptable_evidence\": [\n",
        "        \"A snippet from the 'Product Return SOP v2.1' dated within the last 12 months.\",\n",
        "        \"The specific 'Condition Grade' table mentioned in the technical manual.\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "        \"A direct quote from the policy regarding 'damaged on arrival' items.\",\n",
        "        \"A clear citation link to the source document for agent verification.\"\n",
        "    ],\n",
        "  },\n",
        "  \"U2_high_stakes\": {\n",
        "    \"user_story\": \"As a Compliance Officer, I want the AI to verify that a customer is in a 'permitted jurisdiction' before the agent offers a software license refund so that we avoid violating international trade export laws.\",\n",
        "    \"acceptable_evidence\": [\n",
        "        \"The current 'Global Export Control List' stored in the secure compliance folder.\",\n",
        "        \"The customer's verified account location metadata.\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "        \"A mandatory 'Stop/Proceed' check based on the retrieved legal guideline.\",\n",
        "        \"A warning if the customer's region is flagged as 'Restricted' or 'Sanctioned'.\"\n",
        "    ],\n",
        "  },\n",
        "  \"U3_ambiguous_failure\": {\n",
        "    \"user_story\": \"As a Senior Support Analyst, I want the system to flag a query as 'Unresolved' when the internal manuals contain conflicting instructions so that I can manually intervene before a mistake is made.\",\n",
        "    \"acceptable_evidence\": [\n",
        "        \"Two or more retrieved chunks from different SOPs that provide contradictory steps (e.g., one says 'Refund' and another says 'Store Credit Only').\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "        \"An explicit admission of uncertainty: 'I found conflicting policies in Manual A and Manual B.'\",\n",
        "        \"A request for human escalation rather than a synthesized (hallucinated) compromise.\"\n",
        "    ],\n",
        "}\n",
        "}\n",
        "user_stories"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5189f5",
      "metadata": {
        "id": "8d5189f5"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why U2 is “high-stakes” and what the system must do to avoid harm (abstain, cite evidence, etc.).\n",
        "\n",
        "User story U2 is considered \"high-stakes\" because it involves export control compliance, where a single incorrect response can trigger severe legal and national security repercussions. If the system cannot find a current, definitive export rule for the customer's jurisdiction, it must refuse to answer rather than guess, explicitly stating it lacks the necessary compliance data.The system must cite the exact version and clause of the retrieved compliance document to provide a verifiable audit trail. It should implement a \"hard-stop\" logic where if certain keywords (like \"Sanctioned\") are retrieved, the conversation is automatically escalated to a human compliance officer without generating any further user-facing advice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9c075c",
      "metadata": {
        "id": "3b9c075c"
      },
      "source": [
        "## 1D) Trust & Risk Table (Required)\n",
        "Fill at least **3 rows**. These risks should match your product and user stories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "972f5b88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "972f5b88",
        "outputId": "16bc20e9-baab-4033-830f-3c2ee5502627"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'risk': 'Hallucination',\n",
              "  'example_failure': \"The system retrieves a 2023 'Discount SOP' but 'hallucinates' that a special 2026 COVID-era extension is still active because it saw similar patterns in its training data.\",\n",
              "  'real_world_consequence': 'Direct financial loss from unauthorized payouts and a breakdown in customer trust when the promise cannot be fulfilled.',\n",
              "  'safeguard_idea': \"Force citations + abstain: Use a 'Verification Layer' that checks if the generated answer's dates match the metadata of the retrieved chunks.\"},\n",
              " {'risk': 'Omission',\n",
              "  'example_failure': \"The retriever finds the 'Standard Refund' chunk but misses the 'Hazardous Materials Exception' located in a separate technical appendix.\",\n",
              "  'real_world_consequence': 'Safety or legal violations, such as an analysts inadvertently instructing a customer to mail back a leaking lithium-ion battery.',\n",
              "  'safeguard_idea': \"Recall tuning + hybrid retrieval: Use 'Small-to-Big' chunking where small chunks trigger the retrieval of the entire relevant sub-section (the parent document).\"},\n",
              " {'risk': 'Bias/Misleading',\n",
              "  'example_failure': \"Based on historical (biased) support tickets, the system prioritizes 'Aggressive Upselling' tactics for certain demographics while offering 'Full Refunds' to others.\",\n",
              "  'real_world_consequence': \"Brand reputation damage and potential discriminatory lawsuits under consumer protection or 'Fair Lending' acts.\",\n",
              "  'safeguard_idea': \"Reranking rules + human review: Use a 'Policy-First' reranker that forces the model to prioritize static SOPs over historical conversational patterns.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "risk_table = [\n",
        "  {\n",
        "    \"risk\": \"Hallucination\",\n",
        "    \"example_failure\": \"The system retrieves a 2023 'Discount SOP' but 'hallucinates' that a special 2026 COVID-era extension is still active because it saw similar patterns in its training data.\",\n",
        "    \"real_world_consequence\": \"Direct financial loss from unauthorized payouts and a breakdown in customer trust when the promise cannot be fulfilled.\",\n",
        "    \"safeguard_idea\": \"Force citations + abstain: Use a 'Verification Layer' that checks if the generated answer's dates match the metadata of the retrieved chunks.\"\n",
        "  },\n",
        "  {\n",
        "    \"risk\": \"Omission\",\n",
        "    \"example_failure\": \"The retriever finds the 'Standard Refund' chunk but misses the 'Hazardous Materials Exception' located in a separate technical appendix.\",\n",
        "    \"real_world_consequence\": \"Safety or legal violations, such as an analysts inadvertently instructing a customer to mail back a leaking lithium-ion battery.\",\n",
        "    \"safeguard_idea\": \"Recall tuning + hybrid retrieval: Use 'Small-to-Big' chunking where small chunks trigger the retrieval of the entire relevant sub-section (the parent document).\"\n",
        "  },\n",
        "  {\n",
        "    \"risk\": \"Bias/Misleading\",\n",
        "    \"example_failure\": \"Based on historical (biased) support tickets, the system prioritizes 'Aggressive Upselling' tactics for certain demographics while offering 'Full Refunds' to others.\",\n",
        "    \"real_world_consequence\": \"Brand reputation damage and potential discriminatory lawsuits under consumer protection or 'Fair Lending' acts.\",\n",
        "    \"safeguard_idea\": \"Reranking rules + human review: Use a 'Policy-First' reranker that forces the model to prioritize static SOPs over historical conversational patterns.\"\n",
        "  },\n",
        "]\n",
        "risk_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fe422b",
      "metadata": {
        "id": "33fe422b"
      },
      "source": [
        "✅ **Step 1 Checkpoint (End of Jan 27)**\n",
        "Commit (or submit) your filled templates:\n",
        "- `product`, `dataset_plan`, `user_stories`, `risk_table`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9645a53",
      "metadata": {
        "id": "b9645a53"
      },
      "source": [
        "# STEP 2 — COMPLETION (Jan 29, 60 minutes)\n",
        "**Goal:** Build a working **product-grade** RAG pipeline:\n",
        "Chunking → Keyword + Vector Retrieval → Hybrid α → Governance Rerank → Grounded Answer → Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849ea98a",
      "metadata": {
        "id": "849ea98a"
      },
      "source": [
        "## 2A) Project Dataset Setup (Required for Full Credit)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "\n",
        "### Colab Upload Tips\n",
        "- Left sidebar → **Files** → Upload `.txt`\n",
        "- Place them into `project_data/`\n",
        "\n",
        "This cell creates the folder and shows how many files were found.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a38f48",
      "metadata": {
        "id": "90a38f48"
      },
      "outputs": [],
      "source": [
        "import os, glob, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_FOLDER = \"project_data\"\n",
        "os.makedirs(PROJECT_FOLDER, exist_ok=True)\n",
        "\n",
        "# (Optional helper) Move any .txt in current directory into project_data/\n",
        "moved = 0\n",
        "for fp in glob.glob(\"*.txt\"):\n",
        "    shutil.move(fp, os.path.join(PROJECT_FOLDER, os.path.basename(fp)))\n",
        "    moved += 1\n",
        "\n",
        "files = sorted(glob.glob(os.path.join(PROJECT_FOLDER, \"*.txt\")))\n",
        "print(\"✅ project_data/ ready | moved:\", moved, \"| files:\", len(files))\n",
        "print(\"Example files:\", files[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec380ad4",
      "metadata": {
        "id": "ec380ad4"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "List what dataset you used, how many docs, and why they reflect your product scenario (not just a toy example).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a487a1c7",
      "metadata": {
        "id": "a487a1c7"
      },
      "source": [
        "## 2B) Load Documents + Build Chunks  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This milestone cell loads `.txt` documents and produces chunks using either **fixed** or **semantic** chunking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a081d6",
      "metadata": {
        "id": "13a081d6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def load_project_docs(folder=\"project_data\", max_docs=25):\n",
        "    paths = sorted(Path(folder).glob(\"*.txt\"))[:max_docs]\n",
        "    docs = []\n",
        "    for p in paths:\n",
        "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
        "        if txt:\n",
        "            docs.append({\"doc_id\": p.name, \"text\": txt})\n",
        "    return docs\n",
        "\n",
        "def fixed_chunk(text, chunk_size=900, overlap=150):\n",
        "    # Character-based chunking for speed + simplicity\n",
        "    chunks, i = [], 0\n",
        "    while i < len(text):\n",
        "        chunks.append(text[i:i+chunk_size])\n",
        "        i += (chunk_size - overlap)\n",
        "    return [c.strip() for c in chunks if c.strip()]\n",
        "\n",
        "def semantic_chunk(text, max_chars=1000):\n",
        "    # Paragraph-based packing\n",
        "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "    chunks, cur = [], \"\"\n",
        "    for p in paras:\n",
        "        if len(cur) + len(p) + 2 <= max_chars:\n",
        "            cur = (cur + \"\\n\\n\" + p).strip()\n",
        "        else:\n",
        "            if cur: chunks.append(cur)\n",
        "            cur = p\n",
        "    if cur: chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "# ---- Choose chunking policy ----\n",
        "CHUNKING = \"semantic\"   # \"fixed\" or \"semantic\"\n",
        "FIXED_SIZE = 900\n",
        "FIXED_OVERLAP = 150\n",
        "SEM_MAX = 1000\n",
        "\n",
        "docs = load_project_docs(PROJECT_FOLDER, max_docs=25)\n",
        "print(\"Loaded docs:\", len(docs))\n",
        "\n",
        "all_chunks = []\n",
        "for d in docs:\n",
        "    chunks = fixed_chunk(d[\"text\"], FIXED_SIZE, FIXED_OVERLAP) if CHUNKING == \"fixed\" else semantic_chunk(d[\"text\"], SEM_MAX)\n",
        "    for j, c in enumerate(chunks):\n",
        "        all_chunks.append({\"chunk_id\": f'{d[\"doc_id\"]}::c{j}', \"doc_id\": d[\"doc_id\"], \"text\": c})\n",
        "\n",
        "print(\"Chunking:\", CHUNKING, \"| total chunks:\", len(all_chunks))\n",
        "print(\"Sample chunk id:\", all_chunks[0][\"chunk_id\"] if all_chunks else \"NO CHUNKS (upload .txt files first)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204e5e83",
      "metadata": {
        "id": "204e5e83"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why you chose fixed vs semantic chunking for your product, and how chunking affects precision/recall and trust.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bec9a30",
      "metadata": {
        "id": "9bec9a30"
      },
      "source": [
        "## 2C) Build Retrieval Engines (BM25 + Vector Index)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This cell builds:\n",
        "- **Keyword retrieval** (BM25) for exact matches / compliance\n",
        "- **Vector retrieval** (embeddings + FAISS) for semantic matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0484f1a",
      "metadata": {
        "id": "d0484f1a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ----- Keyword (BM25) -----\n",
        "tokenized = [c[\"text\"].lower().split() for c in all_chunks]\n",
        "bm25 = BM25Okapi(tokenized) if len(tokenized) else None\n",
        "\n",
        "def keyword_search(query, k=10):\n",
        "    if bm25 is None:\n",
        "        return []\n",
        "    scores = bm25.get_scores(query.lower().split())\n",
        "    idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    return [(all_chunks[i], float(scores[i])) for i in idx]\n",
        "\n",
        "# ----- Vector (Embeddings + FAISS) -----\n",
        "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(EMB_MODEL_NAME)\n",
        "\n",
        "chunk_texts = [c[\"text\"] for c in all_chunks]\n",
        "if len(chunk_texts) > 0:\n",
        "    emb = embedder.encode(chunk_texts, show_progress_bar=True, normalize_embeddings=True)\n",
        "    emb = np.asarray(emb, dtype=\"float32\")\n",
        "\n",
        "    index = faiss.IndexFlatIP(emb.shape[1])\n",
        "    index.add(emb)\n",
        "\n",
        "    def vector_search(query, k=10):\n",
        "        q = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
        "        scores, idx = index.search(q, k)\n",
        "        out = [(all_chunks[int(i)], float(s)) for s, i in zip(scores[0], idx[0])]\n",
        "        return out\n",
        "    print(\"✅ Vector index built | chunks:\", len(all_chunks), \"| dim:\", emb.shape[1])\n",
        "else:\n",
        "    index = None\n",
        "    def vector_search(query, k=10): return []\n",
        "    print(\"⚠️ No chunks found. Upload .txt files to project_data/ and rerun.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cb1a14",
      "metadata": {
        "id": "c7cb1a14"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why your product needs both keyword and vector retrieval (what each catches that the other misses).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7dfd29",
      "metadata": {
        "id": "3d7dfd29"
      },
      "source": [
        "## 2D) Hybrid Retrieval (α Fusion Policy)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Hybrid score = **α · keyword + (1 − α) · vector** after simple normalization.\n",
        "\n",
        "Try α ∈ {0.2, 0.5, 0.8} and justify your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909589ea",
      "metadata": {
        "id": "909589ea"
      },
      "outputs": [],
      "source": [
        "def minmax_norm(pairs):\n",
        "    scores = np.array([s for _, s in pairs], dtype=\"float32\") if pairs else np.array([], dtype=\"float32\")\n",
        "    if len(scores) == 0:\n",
        "        return []\n",
        "    mn, mx = float(scores.min()), float(scores.max())\n",
        "    if mx - mn < 1e-8:\n",
        "        return [(c, 1.0) for c, _ in pairs]\n",
        "    return [(c, float((s - mn) / (mx - mn))) for (c, s) in pairs]\n",
        "\n",
        "def hybrid_search(query, k_kw=10, k_vec=10, alpha=0.5, k_out=10):\n",
        "    kw = keyword_search(query, k_kw)\n",
        "    vc = vector_search(query, k_vec)\n",
        "    kw_n = dict((c[\"chunk_id\"], s) for c, s in minmax_norm(kw))\n",
        "    vc_n = dict((c[\"chunk_id\"], s) for c, s in minmax_norm(vc))\n",
        "\n",
        "    ids = set(kw_n) | set(vc_n)\n",
        "    fused = []\n",
        "    for cid in ids:\n",
        "        s = alpha * kw_n.get(cid, 0.0) + (1 - alpha) * vc_n.get(cid, 0.0)\n",
        "        chunk = next(c for c in all_chunks if c[\"chunk_id\"] == cid)\n",
        "        fused.append((chunk, float(s)))\n",
        "\n",
        "    fused.sort(key=lambda x: x[1], reverse=True)\n",
        "    return fused[:k_out]\n",
        "\n",
        "ALPHA = 0.5  # try 0.2 / 0.5 / 0.8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4b3559",
      "metadata": {
        "id": "3a4b3559"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Describe your user type (precision-first vs discovery-first) and why your α choice fits that user and risk profile.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1f888bf",
      "metadata": {
        "id": "b1f888bf"
      },
      "source": [
        "## 2E) Governance Layer (Re-ranking)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Re-ranking is treated as **governance** (risk reduction), not just performance tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e2fb25",
      "metadata": {
        "id": "d8e2fb25"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "RERANK = True\n",
        "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker = CrossEncoder(RERANK_MODEL) if RERANK else None\n",
        "\n",
        "def rerank(query, candidates):\n",
        "    if reranker is None or len(candidates) == 0:\n",
        "        return candidates\n",
        "    pairs = [(query, c[\"text\"]) for c, _ in candidates]\n",
        "    scores = reranker.predict(pairs)\n",
        "    out = [(c, float(s)) for (c, _), s in zip(candidates, scores)]\n",
        "    out.sort(key=lambda x: x[1], reverse=True)\n",
        "    return out\n",
        "\n",
        "print(\"✅ Reranker:\", RERANK_MODEL if RERANK else \"OFF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16bb530f",
      "metadata": {
        "id": "16bb530f"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain what “governance” means for your product and what failure this reranking step helps prevent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81bbbd3",
      "metadata": {
        "id": "d81bbbd3"
      },
      "source": [
        "## 2F) Grounded Answer + Citations  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "We include a lightweight generation option, plus a fallback mode.\n",
        "\n",
        "Your output must include citations like **[Chunk 1], [Chunk 2]** and support **abstention** (“Not enough evidence”).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605ae6d1",
      "metadata": {
        "id": "605ae6d1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "USE_LLM = False  # set True to generate; keep False if downloads are slow\n",
        "GEN_MODEL = \"google/flan-t5-base\"\n",
        "\n",
        "gen = pipeline(\"text2text-generation\", model=GEN_MODEL) if USE_LLM else None\n",
        "\n",
        "def build_context(top_chunks, max_chars=2500):\n",
        "    ctx = \"\"\n",
        "    for i, (c, _) in enumerate(top_chunks, start=1):\n",
        "        block = f\"[Chunk {i}] {c['text'].strip()}\\n\"\n",
        "        if len(ctx) + len(block) > max_chars:\n",
        "            break\n",
        "        ctx += block + \"\\n\"\n",
        "    return ctx.strip()\n",
        "\n",
        "def rag_answer(query, top_chunks):\n",
        "    ctx = build_context(top_chunks)\n",
        "    if USE_LLM and gen is not None:\n",
        "        prompt = (\n",
        "            \"Answer the question using ONLY the evidence below. \"\n",
        "            \"If there is not enough evidence, say 'Not enough evidence.' \"\n",
        "            \"Include citations like [Chunk 1], [Chunk 2].\\n\\n\"\n",
        "            f\"Question: {query}\\n\\nEvidence:\\n{ctx}\\n\\nAnswer:\"\n",
        "        )\n",
        "        out = gen(prompt, max_new_tokens=180)[0][\"generated_text\"]\n",
        "        return out, ctx\n",
        "    else:\n",
        "        # fallback: evidence-first placeholder\n",
        "        answer = (\n",
        "            \"Evidence summary (fallback mode):\\n\"\n",
        "            + \"\\n\".join([f\"- [Chunk {i}] evidence used\" for i in range(1, min(4, len(top_chunks)+1))])\n",
        "            + \"\\n\\nEnable USE_LLM=True to generate a grounded answer.\"\n",
        "        )\n",
        "        return answer, ctx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c50ed74",
      "metadata": {
        "id": "0c50ed74"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain how citations and abstention improve trust in your product, especially for U2 (high-stakes) and U3 (ambiguous).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78586432",
      "metadata": {
        "id": "78586432"
      },
      "source": [
        "## 2G) Run the Pipeline on Your 3 User Stories  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This cell turns your user stories into concrete queries, runs hybrid+rerank, and prints results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "606aaafa",
      "metadata": {
        "id": "606aaafa"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def story_to_query(story_text):\n",
        "    m = re.search(r\"I want to (.+?)(?: so that|\\.|$)\", story_text, flags=re.IGNORECASE)\n",
        "    return m.group(1).strip() if m else story_text.strip()\n",
        "\n",
        "queries = [\n",
        "    (\"U1_normal\", story_to_query(user_stories[\"U1_normal\"][\"user_story\"])),\n",
        "    (\"U2_high_stakes\", story_to_query(user_stories[\"U2_high_stakes\"][\"user_story\"])),\n",
        "    (\"U3_ambiguous_failure\", story_to_query(user_stories[\"U3_ambiguous_failure\"][\"user_story\"])),\n",
        "]\n",
        "\n",
        "def run_pipeline(query, alpha=ALPHA, k=10, do_rerank=RERANK):\n",
        "    base = hybrid_search(query, alpha=alpha, k_out=k)\n",
        "    ranked = rerank(query, base) if do_rerank else base\n",
        "    top5 = ranked[:5]\n",
        "    ans, ctx = rag_answer(query, top5[:3])\n",
        "    return top5, ans, ctx\n",
        "\n",
        "results = {}\n",
        "for key, q in queries:\n",
        "    top5, ans, ctx = run_pipeline(q)\n",
        "    results[key] = {\"query\": q, \"top5\": top5, \"answer\": ans, \"context\": ctx}\n",
        "\n",
        "for key in results:\n",
        "    print(\"\\n===\", key, \"===\")\n",
        "    print(\"Query:\", results[key][\"query\"])\n",
        "    print(\"Top chunk ids:\", [c[\"chunk_id\"] for c, _ in results[key][\"top5\"][:3]])\n",
        "    print(\"Answer preview:\\n\", results[key][\"answer\"][:500], \"...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ae35f7",
      "metadata": {
        "id": "e1ae35f7"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Describe one place where the system helped (better grounding) and one place where it struggled (which layer and why).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b62b369e",
      "metadata": {
        "id": "b62b369e"
      },
      "source": [
        "## 2H) Evaluation (Technical + Product)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Use your rubric to label relevance and compute Precision@5 / Recall@10.\n",
        "Also assign product scores: Trust (1–5) and Decision Confidence (1–5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d7a7869",
      "metadata": {
        "id": "9d7a7869"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(relevant_flags, k=5):\n",
        "    rel = relevant_flags[:k]\n",
        "    return sum(rel) / max(1, len(rel))\n",
        "\n",
        "def recall_at_k(relevant_flags, total_relevant, k=10):\n",
        "    rel_found = sum(relevant_flags[:k])\n",
        "    return rel_found / max(1, total_relevant)\n",
        "\n",
        "evaluation = {}\n",
        "for key in results:\n",
        "    print(\"\\n---\", key, \"---\")\n",
        "    print(\"Query:\", results[key][\"query\"])\n",
        "    print(\"Top-5 chunks:\")\n",
        "    for i, (c, s) in enumerate(results[key][\"top5\"], start=1):\n",
        "        print(i, c[\"chunk_id\"], \"| score:\", round(s, 3))\n",
        "\n",
        "    evaluation[key] = {\n",
        "        \"relevant_flags_top10\": [0]*10,             # set 1 for each relevant chunk among top-10\n",
        "        \"total_relevant_chunks_estimate\": 0,        # estimate from your rubric\n",
        "        \"precision_at_5\": None,\n",
        "        \"recall_at_10\": None,\n",
        "        \"trust_score_1to5\": 0,\n",
        "        \"confidence_score_1to5\": 0,\n",
        "    }\n",
        "\n",
        "evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f1991f",
      "metadata": {
        "id": "92f1991f"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain how you labeled “relevance” using your rubric and what “trust” means for your target users.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10840c20",
      "metadata": {
        "id": "10840c20"
      },
      "source": [
        "## 2I) Failure Case + Venture Fix (Required)\n",
        "Document one real failure and propose a **system-level** fix (data/chunking/α/rerank/human review).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717d394e",
      "metadata": {
        "id": "717d394e"
      },
      "outputs": [],
      "source": [
        "failure_case = {\n",
        "  \"which_user_story\": \"\",\n",
        "  \"what_failed\": \"\",\n",
        "  \"which_layer_failed\": \"\",  # Chunking / Retrieval / Re-ranking / Generation\n",
        "  \"real_world_consequence\": \"\",\n",
        "  \"proposed_system_fix\": \"\",\n",
        "}\n",
        "failure_case\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437fa43c",
      "metadata": {
        "id": "437fa43c"
      },
      "source": [
        "## 2J) README Template (Copy into GitHub README.md)\n",
        "\n",
        "```md\n",
        "# Week 2 Hands-On — Applied RAG Product Results (CS 5588)\n",
        "\n",
        "## Product Overview\n",
        "- Product name:\n",
        "- Target users:\n",
        "- Core problem:\n",
        "- Why RAG:\n",
        "\n",
        "## Dataset Reality\n",
        "- Source / owner:\n",
        "- Sensitivity:\n",
        "- Document types:\n",
        "- Expected scale in production:\n",
        "\n",
        "## User Stories + Rubric\n",
        "- U1:\n",
        "- U2:\n",
        "- U3:\n",
        "(Rubric: acceptable evidence + correct answer criteria)\n",
        "\n",
        "## System Architecture\n",
        "- Chunking:\n",
        "- Keyword retrieval:\n",
        "- Vector retrieval:\n",
        "- Hybrid α:\n",
        "- Reranking governance:\n",
        "- LLM / generation option:\n",
        "\n",
        "## Results\n",
        "| User Story | Method | Precision@5 | Recall@10 | Trust (1–5) | Confidence (1–5) |\n",
        "|---|---|---:|---:|---:|---:|\n",
        "\n",
        "## Failure + Fix\n",
        "- Failure:\n",
        "- Layer:\n",
        "- Consequence:\n",
        "- Safeguard / next fix:\n",
        "\n",
        "## Evidence of Grounding\n",
        "Paste one RAG answer with citations: [Chunk 1], [Chunk 2]\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}