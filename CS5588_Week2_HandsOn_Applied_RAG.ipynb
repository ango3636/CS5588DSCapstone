{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ango3636/CS5588DSCapstone/blob/assignments/CS5588_Week2_HandsOn_Applied_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8beb036f",
      "metadata": {
        "id": "8beb036f"
      },
      "source": [
        "# CS 5588 ‚Äî Week 2 Hands-On: Applied RAG for Product & Venture Development (Two-Step)\n",
        "**Initiation (20 min, Jan 27)** ‚Üí **Completion (60 min, Jan 29)**\n",
        "\n",
        "**Submission:** Survey + GitHub  \n",
        "**Due:** **Jan 29 (Thu), end of class**\n",
        "\n",
        "## New Requirement (Important)\n",
        "For **full credit (2% individual)** you must:\n",
        "1) Use **your own project-aligned dataset** (not only benchmark)  \n",
        "2) Add **your own explanations** for key steps\n",
        "\n",
        "### ‚úÖ ‚ÄúCell Description‚Äù rule (same style as CS 5542)\n",
        "After each **IMPORTANT** code cell, add a short Markdown **Cell Description** (2‚Äì5 sentences):\n",
        "- What the cell does\n",
        "- Why it matters for a **product-grade** RAG system\n",
        "- Any design choices (chunk size, Œ±, reranker, etc.)\n",
        "\n",
        "> Treat these descriptions as **mini system documentation** (engineering + product thinking).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e43e2d",
      "metadata": {
        "id": "d0e43e2d"
      },
      "source": [
        "## Project Dataset Guide (Required for Full Credit)\n",
        "\n",
        "### Minimum requirements\n",
        "- **5‚Äì25 documents** (start small; scale later)\n",
        "- Prefer **plain text** documents (`.txt`)\n",
        "- Put files in a folder named: `project_data/`\n",
        "\n",
        "### Recommended dataset types (choose one)\n",
        "- Policies / guidelines / compliance docs\n",
        "- Technical docs / manuals / SOPs\n",
        "- Customer support FAQs / tickets (de-identified)\n",
        "- Research notes / literature summaries\n",
        "- Domain corpus (healthcare, cybersecurity, business, etc.)\n",
        "\n",
        "> Benchmarks are optional, but **cannot** earn full credit by themselves.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f68d33",
      "metadata": {
        "id": "e7f68d33"
      },
      "source": [
        "## 0) One-Click Setup + Import Check  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "If you are in **Google Colab**, run the install cell below, then **Runtime ‚Üí Restart session** if imports fail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ddaa1c18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddaa1c18",
        "outputId": "95d17608-8284-444c-ad32-cae049e88666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "‚úÖ If imports fail later: Runtime ‚Üí Restart session and run again.\n"
          ]
        }
      ],
      "source": [
        "# CS 5588 Lab 2 ‚Äî One-click dependency install (Colab)\n",
        "!pip -q install -U sentence-transformers chromadb faiss-cpu scikit-learn rank-bm25 transformers accelerate\n",
        "\n",
        "import sys, platform\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"‚úÖ If imports fail later: Runtime ‚Üí Restart session and run again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab532915",
      "metadata": {
        "id": "ab532915"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Write 2‚Äì5 sentences explaining what the setup cell does and why restarting the runtime sometimes matters after pip installs.\n",
        "\n",
        "This setup cell installs and updates all required Python libraries for the lab, including tools for embeddings, vector search, and retrieval models. It then prints the Python and platform information to help verify the runtime environment. Restarting the runtime after pip install is sometimes necessary because newly installed or upgraded packages may not be fully available to the current Python session until it reloads. Restarting ensures that imports use the updated dependencies instead of cached versions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49154e13",
      "metadata": {
        "id": "49154e13"
      },
      "source": [
        "# STEP 1 ‚Äî INITIATION (Jan 27, 20 minutes)\n",
        "**Goal:** Define the **product**, **users**, **dataset reality**, and **trust risks**.\n",
        "\n",
        "> This is a **product milestone**, not a coding demo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58216603",
      "metadata": {
        "id": "58216603"
      },
      "source": [
        "## 1A) Product Framing (Required)  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "Fill in the template below like a founder/product lead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "214ee1ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "214ee1ba",
        "outputId": "861d8695-83ff-4667-ee19-c55c8884d437"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product_name': 'Salesforce Assistant',\n",
              " 'target_users': 'Support Analysts',\n",
              " 'core_problem': \" The 'Execution Gap' between understanding a user and actually solving their problem.\",\n",
              " 'why_rag_not_chatbot': 'Guidelines are stored in a vector database for complex step by step related procedures as well as the ability to provide citations.',\n",
              " 'failure_harms_who_and_how': 'The end user loses trust, the support analyst and the organization loses credibility and is possibly financial discredited.'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "product = {\n",
        "  \"product_name\": \"Salesforce Assistant\",\n",
        "  \"target_users\": \"Support Analysts\",\n",
        "  \"core_problem\": \" The 'Execution Gap' between understanding a user and actually solving their problem.\",\n",
        "  \"why_rag_not_chatbot\": \"Guidelines are stored in a vector database for complex step by step related procedures as well as the ability to provide citations.\",\n",
        "  \"failure_harms_who_and_how\": \"The end user loses trust, the support analyst and the organization loses credibility and is possibly financial discredited.\",\n",
        "}\n",
        "product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "490a084a",
      "metadata": {
        "id": "490a084a"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Explain your product in 3‚Äì5 sentences: who the user is, what pain point exists today, and why grounded RAG helps.\n",
        "\n",
        "Our product is designed for customer support teams and compliance analysts who must navigate complex, rapidly changing internal regulations. Today, these professionals face \"information overload,\" where the gap between a static manual and a live customer conversation leads to inconsistent advice, policy violations, and high burnout. By using grounded RAG, our system ensures every AI-generated response is anchored to your latest verified SOPs, providing a transparent \"audit trail\" that eliminates hallucinations and guarantees that support actions remain both accurate and compliant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "179e8e12",
      "metadata": {
        "id": "179e8e12"
      },
      "source": [
        "## 1B) Dataset Reality Plan (Required)  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "Describe where your data comes from **in the real world**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "282cb6f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "282cb6f9",
        "outputId": "7adfa1b0-71e7-4ee0-ce4b-efdfb0e57562"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_owner': 'Corporate Legal, HR, and Technical Operations Teams',\n",
              " 'data_sensitivity': 'Regulated and Internal (Non-Public)',\n",
              " 'document_types': 'tandard Operating Procedures (SOPs), Product Technical Manuals, and internal Policy Memos',\n",
              " 'expected_scale_in_production': '5k to 50k documents (ranging from 1-page memos to 200-page regulatory filings)',\n",
              " 'data_reality_check_paragraph': \"In the real world, this data is 'messy'‚Äîit exists as scanned PDFs with complex tables, legacy Word docs with conflicting version histories, and internal Wikis that are partially outdated. Unlike a benchmark dataset, these documents often contain contradictory instructions where a new 'Policy Addendum' might override a section of a 'Master SOP' without the original being deleted. Our RAG system must handle this by prioritizing recent version metadata and resolving conflicts through hierarchical retrieval logic.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dataset_plan = {\n",
        "  \"data_owner\": \"Corporate Legal, HR, and Technical Operations Teams\",              # company / agency / public / internal team\n",
        "  \"data_sensitivity\": \"Regulated and Internal (Non-Public)\",        # public / internal / regulated / confidential\n",
        "  \"document_types\": \"tandard Operating Procedures (SOPs), Product Technical Manuals, and internal Policy Memos\",          # policies, manuals, reports, research, etc.\n",
        "  \"expected_scale_in_production\": \"5k to 50k documents (ranging from 1-page memos to 200-page regulatory filings)\",  # e.g., 200 docs, 10k docs, etc.\n",
        "  \"data_reality_check_paragraph\": \"In the real world, this data is 'messy'‚Äîit exists as scanned PDFs with complex tables, legacy Word docs with conflicting version histories, and internal Wikis that are partially outdated. Unlike a benchmark dataset, these documents often contain contradictory instructions where a new 'Policy Addendum' might override a section of a 'Master SOP' without the original being deleted. Our RAG system must handle this by prioritizing recent version metadata and resolving conflicts through hierarchical retrieval logic.\",\n",
        "}\n",
        "dataset_plan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e2da001",
      "metadata": {
        "id": "3e2da001"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Write 2‚Äì5 sentences describing where this data would come from in a real deployment and any privacy/regulatory constraints.\n",
        "\n",
        "An example of where this data could come from is Salesforce. As a support analyst, I utilize saleforce which contains old cases reported by users that contain comments and email conversation, photo attachments, case resolutions as well as knowledge based articles and known issues such as bugs, enhancements, or tasks. This type of data is private to the company and their solutions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df3ac72",
      "metadata": {
        "id": "2df3ac72"
      },
      "source": [
        "## 1C) User Stories + Mini Rubric (Required)  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "Define **3 user stories** (U1 normal, U2 high-stakes, U3 ambiguous/failure) + rubric for evidence and correctness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0a72b8eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a72b8eb",
        "outputId": "cfb1100d-5ca1-4e62-82e3-d78a32bf0879"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'U1_normal': {'user_story': 'As a Junior Support Agent, I want to quickly retrieve the specific return policy for a damaged item so that I can provide an accurate response without searching through 50+ PDFs.',\n",
              "  'acceptable_evidence': [\"A snippet from the 'Product Return SOP v2.1' dated within the last 12 months.\",\n",
              "   \"The specific 'Condition Grade' table mentioned in the technical manual.\"],\n",
              "  'correct_answer_must_include': [\"A direct quote from the policy regarding 'damaged on arrival' items.\",\n",
              "   'A clear citation link to the source document for agent verification.']},\n",
              " 'U2_high_stakes': {'user_story': \"As a Compliance Officer, I want the AI to verify that a customer is in a 'permitted jurisdiction' before the agent offers a software license refund so that we avoid violating international trade export laws.\",\n",
              "  'acceptable_evidence': [\"The current 'Global Export Control List' stored in the secure compliance folder.\",\n",
              "   \"The customer's verified account location metadata.\"],\n",
              "  'correct_answer_must_include': [\"A mandatory 'Stop/Proceed' check based on the retrieved legal guideline.\",\n",
              "   \"A warning if the customer's region is flagged as 'Restricted' or 'Sanctioned'.\"]},\n",
              " 'U3_ambiguous_failure': {'user_story': \"As a Senior Support Analyst, I want the system to flag a query as 'Unresolved' when the internal manuals contain conflicting instructions so that I can manually intervene before a mistake is made.\",\n",
              "  'acceptable_evidence': [\"Two or more retrieved chunks from different SOPs that provide contradictory steps (e.g., one says 'Refund' and another says 'Store Credit Only').\"],\n",
              "  'correct_answer_must_include': [\"An explicit admission of uncertainty: 'I found conflicting policies in Manual A and Manual B.'\",\n",
              "   'A request for human escalation rather than a synthesized (hallucinated) compromise.']}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "user_stories = {\n",
        "  \"U1_normal\": {\n",
        "    \"user_story\": \"As a Junior Support Agent, I want to quickly retrieve the specific return policy for a damaged item so that I can provide an accurate response without searching through 50+ PDFs.\",\n",
        "    \"acceptable_evidence\": [\n",
        "        \"A snippet from the 'Product Return SOP v2.1' dated within the last 12 months.\",\n",
        "        \"The specific 'Condition Grade' table mentioned in the technical manual.\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "        \"A direct quote from the policy regarding 'damaged on arrival' items.\",\n",
        "        \"A clear citation link to the source document for agent verification.\"\n",
        "    ],\n",
        "  },\n",
        "  \"U2_high_stakes\": {\n",
        "    \"user_story\": \"As a Compliance Officer, I want the AI to verify that a customer is in a 'permitted jurisdiction' before the agent offers a software license refund so that we avoid violating international trade export laws.\",\n",
        "    \"acceptable_evidence\": [\n",
        "        \"The current 'Global Export Control List' stored in the secure compliance folder.\",\n",
        "        \"The customer's verified account location metadata.\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "        \"A mandatory 'Stop/Proceed' check based on the retrieved legal guideline.\",\n",
        "        \"A warning if the customer's region is flagged as 'Restricted' or 'Sanctioned'.\"\n",
        "    ],\n",
        "  },\n",
        "  \"U3_ambiguous_failure\": {\n",
        "    \"user_story\": \"As a Senior Support Analyst, I want the system to flag a query as 'Unresolved' when the internal manuals contain conflicting instructions so that I can manually intervene before a mistake is made.\",\n",
        "    \"acceptable_evidence\": [\n",
        "        \"Two or more retrieved chunks from different SOPs that provide contradictory steps (e.g., one says 'Refund' and another says 'Store Credit Only').\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "        \"An explicit admission of uncertainty: 'I found conflicting policies in Manual A and Manual B.'\",\n",
        "        \"A request for human escalation rather than a synthesized (hallucinated) compromise.\"\n",
        "    ],\n",
        "}\n",
        "}\n",
        "user_stories"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5189f5",
      "metadata": {
        "id": "8d5189f5"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Explain why U2 is ‚Äúhigh-stakes‚Äù and what the system must do to avoid harm (abstain, cite evidence, etc.).\n",
        "\n",
        "User story U2 is considered \"high-stakes\" because it involves export control compliance, where a single incorrect response can trigger severe legal and national security repercussions. If the system cannot find a current, definitive export rule for the customer's jurisdiction, it must refuse to answer rather than guess, explicitly stating it lacks the necessary compliance data.The system must cite the exact version and clause of the retrieved compliance document to provide a verifiable audit trail. It should implement a \"hard-stop\" logic where if certain keywords (like \"Sanctioned\") are retrieved, the conversation is automatically escalated to a human compliance officer without generating any further user-facing advice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9c075c",
      "metadata": {
        "id": "3b9c075c"
      },
      "source": [
        "## 1D) Trust & Risk Table (Required)\n",
        "Fill at least **3 rows**. These risks should match your product and user stories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "972f5b88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "972f5b88",
        "outputId": "892d2a52-a7b2-40cc-ff4a-39b8a771359d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'risk': 'Hallucination',\n",
              "  'example_failure': \"The system retrieves a 2023 'Discount SOP' but 'hallucinates' that a special 2026 COVID-era extension is still active because it saw similar patterns in its training data.\",\n",
              "  'real_world_consequence': 'Direct financial loss from unauthorized payouts and a breakdown in customer trust when the promise cannot be fulfilled.',\n",
              "  'safeguard_idea': \"Force citations + abstain: Use a 'Verification Layer' that checks if the generated answer's dates match the metadata of the retrieved chunks.\"},\n",
              " {'risk': 'Omission',\n",
              "  'example_failure': \"The retriever finds the 'Standard Refund' chunk but misses the 'Hazardous Materials Exception' located in a separate technical appendix.\",\n",
              "  'real_world_consequence': 'Safety or legal violations, such as an analysts inadvertently instructing a customer to mail back a leaking lithium-ion battery.',\n",
              "  'safeguard_idea': \"Recall tuning + hybrid retrieval: Use 'Small-to-Big' chunking where small chunks trigger the retrieval of the entire relevant sub-section (the parent document).\"},\n",
              " {'risk': 'Bias/Misleading',\n",
              "  'example_failure': \"Based on historical (biased) support tickets, the system prioritizes 'Aggressive Upselling' tactics for certain demographics while offering 'Full Refunds' to others.\",\n",
              "  'real_world_consequence': \"Brand reputation damage and potential discriminatory lawsuits under consumer protection or 'Fair Lending' acts.\",\n",
              "  'safeguard_idea': \"Reranking rules + human review: Use a 'Policy-First' reranker that forces the model to prioritize static SOPs over historical conversational patterns.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "risk_table = [\n",
        "  {\n",
        "    \"risk\": \"Hallucination\",\n",
        "    \"example_failure\": \"The system retrieves a 2023 'Discount SOP' but 'hallucinates' that a special 2026 COVID-era extension is still active because it saw similar patterns in its training data.\",\n",
        "    \"real_world_consequence\": \"Direct financial loss from unauthorized payouts and a breakdown in customer trust when the promise cannot be fulfilled.\",\n",
        "    \"safeguard_idea\": \"Force citations + abstain: Use a 'Verification Layer' that checks if the generated answer's dates match the metadata of the retrieved chunks.\"\n",
        "  },\n",
        "  {\n",
        "    \"risk\": \"Omission\",\n",
        "    \"example_failure\": \"The retriever finds the 'Standard Refund' chunk but misses the 'Hazardous Materials Exception' located in a separate technical appendix.\",\n",
        "    \"real_world_consequence\": \"Safety or legal violations, such as an analysts inadvertently instructing a customer to mail back a leaking lithium-ion battery.\",\n",
        "    \"safeguard_idea\": \"Recall tuning + hybrid retrieval: Use 'Small-to-Big' chunking where small chunks trigger the retrieval of the entire relevant sub-section (the parent document).\"\n",
        "  },\n",
        "  {\n",
        "    \"risk\": \"Bias/Misleading\",\n",
        "    \"example_failure\": \"Based on historical (biased) support tickets, the system prioritizes 'Aggressive Upselling' tactics for certain demographics while offering 'Full Refunds' to others.\",\n",
        "    \"real_world_consequence\": \"Brand reputation damage and potential discriminatory lawsuits under consumer protection or 'Fair Lending' acts.\",\n",
        "    \"safeguard_idea\": \"Reranking rules + human review: Use a 'Policy-First' reranker that forces the model to prioritize static SOPs over historical conversational patterns.\"\n",
        "  },\n",
        "]\n",
        "risk_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fe422b",
      "metadata": {
        "id": "33fe422b"
      },
      "source": [
        "‚úÖ **Step 1 Checkpoint (End of Jan 27)**\n",
        "Commit (or submit) your filled templates:\n",
        "- `product`, `dataset_plan`, `user_stories`, `risk_table`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9645a53",
      "metadata": {
        "id": "b9645a53"
      },
      "source": [
        "# STEP 2 ‚Äî COMPLETION (Jan 29, 60 minutes)\n",
        "**Goal:** Build a working **product-grade** RAG pipeline:\n",
        "Chunking ‚Üí Keyword + Vector Retrieval ‚Üí Hybrid Œ± ‚Üí Governance Rerank ‚Üí Grounded Answer ‚Üí Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849ea98a",
      "metadata": {
        "id": "849ea98a"
      },
      "source": [
        "## 2A) Project Dataset Setup (Required for Full Credit)  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "\n",
        "### Colab Upload Tips\n",
        "- Left sidebar ‚Üí **Files** ‚Üí Upload `.txt`\n",
        "- Place them into `project_data/`\n",
        "\n",
        "This cell creates the folder and shows how many files were found.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec380ad4",
      "metadata": {
        "id": "ec380ad4"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "List what dataset you used, how many docs, and why they reflect your product scenario (not just a toy example).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm # Progress bar\n",
        "\n",
        "# 1. Setup Folder\n",
        "PROJECT_FOLDER = \"project_data\"\n",
        "if os.path.exists(PROJECT_FOLDER):\n",
        "    shutil.rmtree(PROJECT_FOLDER)\n",
        "os.makedirs(PROJECT_FOLDER, exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Loading the full Bitext dataset into memory...\")\n",
        "try:\n",
        "    dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\", split=\"train\")\n",
        "\n",
        "    # You can change this limit to len(dataset) for the absolute full set\n",
        "    limit = 26000\n",
        "    print(f\"üìÑ Writing {limit} tickets into categorized folders...\")\n",
        "\n",
        "    for i in tqdm(range(limit)):\n",
        "        entry = dataset[i]\n",
        "        category = entry['category'].replace(\" \", \"_\")\n",
        "\n",
        "        category_path = os.path.join(PROJECT_FOLDER, category)\n",
        "        if not os.path.exists(category_path):\n",
        "            os.makedirs(category_path, exist_ok=True)\n",
        "\n",
        "        file_path = os.path.join(category_path, f\"ticket_{i:04d}.txt\")\n",
        "\n",
        "        with open(file_path, \"w\") as f:\n",
        "            f.write(f\"TICKET_{i:04d}\\nINTENT: {entry['intent']}\\n\\nUSER: {entry['instruction']}\\n\\nRESPONSE: {entry['response']}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Done! Dataset ready in '{PROJECT_FOLDER}'\")\n",
        "    print(f\"Categorized folders created: {os.listdir(PROJECT_FOLDER)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ],
      "metadata": {
        "id": "y4VtL2tJLxPF"
      },
      "id": "y4VtL2tJLxPF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a487a1c7",
      "metadata": {
        "id": "a487a1c7"
      },
      "source": [
        "## 2B) Load Documents + Build Chunks  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "This milestone cell loads `.txt` documents and produces chunks using either **fixed** or **semantic** chunking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "13a081d6",
      "metadata": {
        "id": "13a081d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953f824b-510e-4dc0-e80f-afda0a100eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded docs: 25\n",
            "Chunking: semantic | total chunks: 26\n",
            "Sample chunk id: ACCOUNT/ticket_10000.txt::c0\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def load_project_docs(folder=\"project_data\", max_docs=25):\n",
        "    # .rglob(\"*.txt\") finds all .txt files in subdirectories recursively\n",
        "    paths = sorted(Path(folder).rglob(\"*.txt\"))[:max_docs]\n",
        "    docs = []\n",
        "    for p in paths:\n",
        "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
        "        if txt:\n",
        "            # We use the folder name + filename as the ID so you know the category\n",
        "            doc_id = f\"{p.parent.name}/{p.name}\"\n",
        "            docs.append({\"doc_id\": doc_id, \"text\": txt})\n",
        "    return docs\n",
        "\n",
        "def fixed_chunk(text, chunk_size=900, overlap=150):\n",
        "    # Character-based chunking for speed + simplicity\n",
        "    chunks, i = [], 0\n",
        "    while i < len(text):\n",
        "        chunks.append(text[i:i+chunk_size])\n",
        "        i += (chunk_size - overlap)\n",
        "    return [c.strip() for c in chunks if c.strip()]\n",
        "\n",
        "def semantic_chunk(text, max_chars=1000):\n",
        "    # Paragraph-based packing\n",
        "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "    chunks, cur = [], \"\"\n",
        "    for p in paras:\n",
        "        if len(cur) + len(p) + 2 <= max_chars:\n",
        "            cur = (cur + \"\\n\\n\" + p).strip()\n",
        "        else:\n",
        "            if cur: chunks.append(cur)\n",
        "            cur = p\n",
        "    if cur: chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "# ---- Choose chunking policy ----\n",
        "CHUNKING = \"semantic\"   # \"fixed\" or \"semantic\"\n",
        "FIXED_SIZE = 900\n",
        "FIXED_OVERLAP = 150\n",
        "SEM_MAX = 1000\n",
        "\n",
        "docs = load_project_docs(PROJECT_FOLDER, max_docs=25)\n",
        "print(\"Loaded docs:\", len(docs))\n",
        "\n",
        "all_chunks = []\n",
        "for d in docs:\n",
        "    chunks = fixed_chunk(d[\"text\"], FIXED_SIZE, FIXED_OVERLAP) if CHUNKING == \"fixed\" else semantic_chunk(d[\"text\"], SEM_MAX)\n",
        "    for j, c in enumerate(chunks):\n",
        "        all_chunks.append({\"chunk_id\": f'{d[\"doc_id\"]}::c{j}', \"doc_id\": d[\"doc_id\"], \"text\": c})\n",
        "\n",
        "print(\"Chunking:\", CHUNKING, \"| total chunks:\", len(all_chunks))\n",
        "print(\"Sample chunk id:\", all_chunks[0][\"chunk_id\"] if all_chunks else \"NO CHUNKS (upload .txt files first)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204e5e83",
      "metadata": {
        "id": "204e5e83"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Explain why you chose fixed vs semantic chunking for your product, and how chunking affects precision/recall and trust.\n",
        "\n",
        "Choosing semantic chunking is the strategic choice for a support analyst tool because technical tickets are highly structured, typically following a \"Problem-Diagnosis-Solution\" flow. Unlike fixed chunking, which might split a critical error code or a step-by-step fix in half, semantic chunking preserves the integrity of these paragraphs, ensuring that the precision of your search results is higher by only returning complete, relevant thoughts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bec9a30",
      "metadata": {
        "id": "9bec9a30"
      },
      "source": [
        "## 2C) Build Retrieval Engines (BM25 + Vector Index)  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "This cell builds:\n",
        "- **Keyword retrieval** (BM25) for exact matches / compliance\n",
        "- **Vector retrieval** (embeddings + FAISS) for semantic matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0484f1a",
      "metadata": {
        "id": "d0484f1a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ----- Keyword (BM25) -----\n",
        "tokenized = [c[\"text\"].lower().split() for c in all_chunks]\n",
        "bm25 = BM25Okapi(tokenized) if len(tokenized) else None\n",
        "\n",
        "def keyword_search(query, k=10):\n",
        "    if bm25 is None:\n",
        "        return []\n",
        "    scores = bm25.get_scores(query.lower().split())\n",
        "    idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    return [(all_chunks[i], float(scores[i])) for i in idx]\n",
        "\n",
        "# ----- Vector (Embeddings + FAISS) -----\n",
        "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(EMB_MODEL_NAME)\n",
        "\n",
        "chunk_texts = [c[\"text\"] for c in all_chunks]\n",
        "if len(chunk_texts) > 0:\n",
        "    emb = embedder.encode(chunk_texts, show_progress_bar=True, normalize_embeddings=True)\n",
        "    emb = np.asarray(emb, dtype=\"float32\")\n",
        "\n",
        "    index = faiss.IndexFlatIP(emb.shape[1])\n",
        "    index.add(emb)\n",
        "\n",
        "    def vector_search(query, k=10):\n",
        "        q = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
        "        scores, idx = index.search(q, k)\n",
        "        out = [(all_chunks[int(i)], float(s)) for s, i in zip(scores[0], idx[0])]\n",
        "        return out\n",
        "    print(\"‚úÖ Vector index built | chunks:\", len(all_chunks), \"| dim:\", emb.shape[1])\n",
        "else:\n",
        "    index = None\n",
        "    def vector_search(query, k=10): return []\n",
        "    print(\"‚ö†Ô∏è No chunks found. Upload .txt files to project_data/ and rerun.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cb1a14",
      "metadata": {
        "id": "c7cb1a14"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Explain why your product needs both keyword and vector retrieval (what each catches that the other misses).\n",
        "\n",
        "In a professional support analyst product, you need both keyword and vector retrieval because they solve different \"blind spots\" in technical troubleshooting.\n",
        "\n",
        "Keyword search (BM25) is essential for precision when an analyst searches for specific technical identifiers like error codes (e.g., \"Error 404\"), product version numbers, or rare acronyms that a vector model might accidentally \"smooth over\" into a more common concept.\n",
        "\n",
        "Vector search (FAISS), on the other hand, excels at semantic understanding and intent; it can find a solution for a \"blank screen\" even if the support ticket only uses terms like \"display issues\" or \"monitor not turning on.\"\n",
        "\n",
        "By combining both, you ensure the system is both literally accurate (catching the exact technical details) and conceptually smart (finding relevant fixes even when the wording varies). This hybrid approach significantly increases recall by catching all potential solutions and boosts trust, as the AI avoids \"hallucinating\" a generic answer when a specific, keyword-rich technical manual exists.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7dfd29",
      "metadata": {
        "id": "3d7dfd29"
      },
      "source": [
        "## 2D) Hybrid Retrieval (Œ± Fusion Policy)  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "Hybrid score = **Œ± ¬∑ keyword + (1 ‚àí Œ±) ¬∑ vector** after simple normalization.\n",
        "\n",
        "Try Œ± ‚àà {0.2, 0.5, 0.8} and justify your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "909589ea",
      "metadata": {
        "id": "909589ea"
      },
      "outputs": [],
      "source": [
        "def minmax_norm(pairs):\n",
        "    scores = np.array([s for _, s in pairs], dtype=\"float32\") if pairs else np.array([], dtype=\"float32\")\n",
        "    if len(scores) == 0:\n",
        "        return []\n",
        "    mn, mx = float(scores.min()), float(scores.max())\n",
        "    if mx - mn < 1e-8:\n",
        "        return [(c, 1.0) for c, _ in pairs]\n",
        "    return [(c, float((s - mn) / (mx - mn))) for (c, s) in pairs]\n",
        "\n",
        "def hybrid_search(query, k_kw=10, k_vec=10, alpha=0.5, k_out=10):\n",
        "    kw = keyword_search(query, k_kw)\n",
        "    vc = vector_search(query, k_vec)\n",
        "    kw_n = dict((c[\"chunk_id\"], s) for c, s in minmax_norm(kw))\n",
        "    vc_n = dict((c[\"chunk_id\"], s) for c, s in minmax_norm(vc))\n",
        "\n",
        "    ids = set(kw_n) | set(vc_n)\n",
        "    fused = []\n",
        "    for cid in ids:\n",
        "        s = alpha * kw_n.get(cid, 0.0) + (1 - alpha) * vc_n.get(cid, 0.0)\n",
        "        chunk = next(c for c in all_chunks if c[\"chunk_id\"] == cid)\n",
        "        fused.append((chunk, float(s)))\n",
        "\n",
        "    fused.sort(key=lambda x: x[1], reverse=True)\n",
        "    return fused[:k_out]\n",
        "\n",
        "ALPHA = 0.5  # try 0.2 / 0.5 / 0.8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4b3559",
      "metadata": {
        "id": "3a4b3559"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Describe your user type (precision-first vs discovery-first) and why your Œ± choice fits that user and risk profile.\n",
        "\n",
        "Selecting $\\alpha = 0.5$ for a support analyst tool provides a balanced hybrid approach that ensures high performance across both technical and conversational queries. This setting is optimal because it prevents the \"fuzziness\" of vector search from burying exact technical identifiers‚Äîlike error codes (e.g., GFX-108) or specific product names‚Äîwhich keyword search (BM25) excels at retrieving. Simultaneously, it leverages the vector component to catch \"concept-seeking\" queries where users describe issues in natural language, such as \"screen is blank,\" even if the relevant ticket only mentions \"display troubleshooting.\" By giving equal weight to both literal accuracy and semantic intent, the system maximizes recall without sacrificing the precision needed for technical reliability. This balance is crucial for building analyst trust, as it consistently delivers results that are both contextually smart and technically pinpointed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1f888bf",
      "metadata": {
        "id": "b1f888bf"
      },
      "source": [
        "## 2E) Governance Layer (Re-ranking)  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "Re-ranking is treated as **governance** (risk reduction), not just performance tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e2fb25",
      "metadata": {
        "id": "d8e2fb25"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "RERANK = True\n",
        "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker = CrossEncoder(RERANK_MODEL) if RERANK else None\n",
        "\n",
        "def rerank(query, candidates):\n",
        "    if reranker is None or len(candidates) == 0:\n",
        "        return candidates\n",
        "    pairs = [(query, c[\"text\"]) for c, _ in candidates]\n",
        "    scores = reranker.predict(pairs)\n",
        "    out = [(c, float(s)) for (c, _), s in zip(candidates, scores)]\n",
        "    out.sort(key=lambda x: x[1], reverse=True)\n",
        "    return out\n",
        "\n",
        "print(\"‚úÖ Reranker:\", RERANK_MODEL if RERANK else \"OFF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16bb530f",
      "metadata": {
        "id": "16bb530f"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Explain what ‚Äúgovernance‚Äù means for your product and what failure this reranking step helps prevent.\n",
        "\n",
        "In this context, governance means enforcing a strict layer of validation to ensure the most technically accurate and safe solution is presented to the analyst, rather than just the most \"popular\" or \"similar\" one.\n",
        "\n",
        "While the initial hybrid search is fast, it can occasionally suffer from semantic drift, where a vector model retrieves a ticket that looks similar in tone but describes a completely different technical fix. The cross-encoder reranker prevents this failure by performing a deep, pairwise comparison between the user's specific problem and the retrieved document, acting as a \"fact-checker\" that filters out high-scoring but irrelevant noise.\n",
        "\n",
        "By re-evaluating the top candidates, the reranker significantly reduces the risk of misinformation‚Äîpreventing an analyst from applying a fix for \"User Account Lockout\" to a \"Server Security Breach\" just because both tickets shared high-level security keywords. This extra step transforms the system from a simple search engine into a reliable decision-support tool that analysts can trust for critical operations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81bbbd3",
      "metadata": {
        "id": "d81bbbd3"
      },
      "source": [
        "## 2F) Grounded Answer + Citations  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "We include a lightweight generation option, plus a fallback mode.\n",
        "\n",
        "Your output must include citations like **[Chunk 1], [Chunk 2]** and support **abstention** (‚ÄúNot enough evidence‚Äù).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "605ae6d1",
      "metadata": {
        "id": "605ae6d1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "USE_LLM = False  # set True to generate; keep False if downloads are slow\n",
        "GEN_MODEL = \"google/flan-t5-base\"\n",
        "\n",
        "gen = pipeline(\"text2text-generation\", model=GEN_MODEL) if USE_LLM else None\n",
        "\n",
        "def build_context(top_chunks, max_chars=2500):\n",
        "    ctx = \"\"\n",
        "    for i, (c, _) in enumerate(top_chunks, start=1):\n",
        "        block = f\"[Chunk {i}] {c['text'].strip()}\\n\"\n",
        "        if len(ctx) + len(block) > max_chars:\n",
        "            break\n",
        "        ctx += block + \"\\n\"\n",
        "    return ctx.strip()\n",
        "\n",
        "def rag_answer(query, top_chunks):\n",
        "\n",
        "  # Extract just the text from the list of (chunk_dict, score) tuples\n",
        "    context_text = \"\\n\\n\".join([c[\"text\"] for c, score in top_chunks])\n",
        "\n",
        "    if not USE_LLM:\n",
        "        # If LLM is off, at least return the raw text so you can see it\n",
        "        evidence = \"\\n\".join([f\"- [Chunk {i}] {c['text'][:100]}...\" for i, (c, s) in enumerate(top_chunks)])\n",
        "        return f\"PROMPT MODE (No LLM):\\n{evidence}\", context_text\n",
        "\n",
        "    ctx = build_context(top_chunks)\n",
        "    if USE_LLM and gen is not None:\n",
        "        prompt = (\n",
        "            \"Answer the question using ONLY the evidence below. \"\n",
        "            \"If there is not enough evidence, say 'Not enough evidence.' \"\n",
        "            \"Include citations like [Chunk 1], [Chunk 2].\\n\\n\"\n",
        "            f\"Question: {query}\\n\\nEvidence:\\n{ctx}\\n\\nAnswer:\"\n",
        "        )\n",
        "        out = gen(prompt, max_new_tokens=180)[0][\"generated_text\"]\n",
        "        return out, ctx\n",
        "    else:\n",
        "        # fallback: evidence-first placeholder\n",
        "        answer = (\n",
        "            \"Evidence summary (fallback mode):\\n\"\n",
        "            + \"\\n\".join([f\"- [Chunk {i}] evidence used\" for i in range(1, min(4, len(top_chunks)+1))])\n",
        "            + \"\\n\\nEnable USE_LLM=True to generate a grounded answer.\"\n",
        "        )\n",
        "        return answer, ctx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c50ed74",
      "metadata": {
        "id": "0c50ed74"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Explain how citations and abstention improve trust in your product, especially for U2 (high-stakes) and U3 (ambiguous).\n",
        "\n",
        "In a support analyst environment, citations and abstention act as the primary safety rails that transition the AI from a \"black box\" into a verifiable tool. For U2 (High-Stakes) scenarios, such as resetting a firewall or handling sensitive account data, citations provide a direct link to the source ticket or manual. This allows the analyst to perform \"human-in-the-loop\" verification, ensuring they aren't blindly following an AI suggestion for a high-risk operation. By showing exactly where the information came from, the product builds trust through transparency and accountability.\n",
        "\n",
        "For U3 (Ambiguous) queries, where the user‚Äôs problem is poorly defined or the database lacks a clear fix, abstention is critical. Instead of \"hallucinating\" a generic answer or providing a low-confidence guess that could lead to a catastrophic error, the system simply admits it doesn't know. This prevents the specific failure of overconfidence, where a system provides a wrong answer that looks right. For an analyst, a system that says \"I cannot find a confident match for this error\" is far more trustworthy than one that consistently provides irrelevant or incorrect advice."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78586432",
      "metadata": {
        "id": "78586432"
      },
      "source": [
        "## 2G) Run the Pipeline on Your 3 User Stories  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "This cell turns your user stories into concrete queries, runs hybrid+rerank, and prints results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "606aaafa",
      "metadata": {
        "id": "606aaafa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34202d4e-7acf-4e64-ddb8-f8ed3556b016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== U1_normal ===\n",
            "Query: quickly retrieve the specific return policy for a damaged item\n",
            "Top chunk ids: ['ACCOUNT/ticket_10015.txt::c0', 'ACCOUNT/ticket_10014.txt::c0', 'ACCOUNT/ticket_10010.txt::c0']\n",
            "Answer preview:\n",
            " PROMPT MODE (No LLM):\n",
            "- [Chunk 0] TICKET_10015\n",
            "INTENT: create_account\n",
            "\n",
            "USER: i have a problem with creating a {{Account Category}} acc...\n",
            "- [Chunk 1] TICKET_10014\n",
            "INTENT: create_account\n",
            "\n",
            "USER: can I open a freemium account?\n",
            "\n",
            "RESPONSE: Sure! üòä I'm del...\n",
            "- [Chunk 2] TICKET_10010\n",
            "INTENT: create_account\n",
            "\n",
            "USER: tell me more about opening premium accounts\n",
            "\n",
            "RESPONSE: Th... ...\n",
            "\n",
            "\n",
            "=== U2_high_stakes ===\n",
            "Query: As a Compliance Officer, I want the AI to verify that a customer is in a 'permitted jurisdiction' before the agent offers a software license refund so that we avoid violating international trade export laws.\n",
            "Top chunk ids: ['ACCOUNT/ticket_10001.txt::c0', 'ACCOUNT/ticket_10010.txt::c0', 'ACCOUNT/ticket_10024.txt::c0']\n",
            "Answer preview:\n",
            " PROMPT MODE (No LLM):\n",
            "- [Chunk 0] TICKET_10001\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I do not have a freemium account, I try to register\n",
            "\n",
            "RESP...\n",
            "- [Chunk 1] TICKET_10010\n",
            "INTENT: create_account\n",
            "\n",
            "USER: tell me more about opening premium accounts\n",
            "\n",
            "RESPONSE: Th...\n",
            "- [Chunk 2] TICKET_10024\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I don't know what to do to create a {{Account Type}} acco... ...\n",
            "\n",
            "\n",
            "=== U3_ambiguous_failure ===\n",
            "Query: As a Senior Support Analyst, I want the system to flag a query as 'Unresolved' when the internal manuals contain conflicting instructions so that I can manually intervene before a mistake is made.\n",
            "Top chunk ids: ['ACCOUNT/ticket_10011.txt::c0', 'ACCOUNT/ticket_10001.txt::c0', 'ACCOUNT/ticket_10015.txt::c0']\n",
            "Answer preview:\n",
            " PROMPT MODE (No LLM):\n",
            "- [Chunk 0] TICKET_10011\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I have a question about opening standard accounts\n",
            "\n",
            "RESPON...\n",
            "- [Chunk 1] TICKET_10001\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I do not have a freemium account, I try to register\n",
            "\n",
            "RESP...\n",
            "- [Chunk 2] TICKET_10015\n",
            "INTENT: create_account\n",
            "\n",
            "USER: i have a problem with creating a {{Account Category}} acc... ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def story_to_query(story_text):\n",
        "    m = re.search(r\"I want to (.+?)(?: so that|\\.|$)\", story_text, flags=re.IGNORECASE)\n",
        "    return m.group(1).strip() if m else story_text.strip()\n",
        "\n",
        "queries = [\n",
        "    (\"U1_normal\", story_to_query(user_stories[\"U1_normal\"][\"user_story\"])),\n",
        "    (\"U2_high_stakes\", story_to_query(user_stories[\"U2_high_stakes\"][\"user_story\"])),\n",
        "    (\"U3_ambiguous_failure\", story_to_query(user_stories[\"U3_ambiguous_failure\"][\"user_story\"])),\n",
        "]\n",
        "\n",
        "def run_pipeline(query, alpha=ALPHA, k=10, do_rerank=RERANK):\n",
        "    base = hybrid_search(query, alpha=alpha, k_out=k)\n",
        "    ranked = rerank(query, base) if do_rerank else base\n",
        "    top5 = ranked[:5]\n",
        "    ans, ctx = rag_answer(query, top5[:3])\n",
        "    return top5, ans, ctx\n",
        "\n",
        "results = {}\n",
        "for key, q in queries:\n",
        "    top5, ans, ctx = run_pipeline(q)\n",
        "    results[key] = {\"query\": q, \"top5\": top5, \"answer\": ans, \"context\": ctx}\n",
        "\n",
        "for key in results:\n",
        "    print(\"\\n===\", key, \"===\")\n",
        "    print(\"Query:\", results[key][\"query\"])\n",
        "    print(\"Top chunk ids:\", [c[\"chunk_id\"] for c, _ in results[key][\"top5\"][:3]])\n",
        "    print(\"Answer preview:\\n\", results[key][\"answer\"][:500], \"...\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def story_to_query(story_text):\n",
        "    m = re.search(r\"I want to (.+?)(?: so that|\\.|$)\", story_text, flags=re.IGNORECASE)\n",
        "    return m.group(1).strip() if m else story_text.strip()\n",
        "\n",
        "queries = [\n",
        "    (\"U1_normal\", story_to_query(user_stories[\"U1_normal\"][\"user_story\"])),\n",
        "    (\"U2_high_stakes\", story_to_query(user_stories[\"U2_high_stakes\"][\"user_story\"])),\n",
        "    (\"U3_ambiguous_failure\", story_to_query(user_stories[\"U3_ambiguous_failure\"][\"user_story\"])),\n",
        "]\n",
        "\n",
        "def run_pipeline(query, alpha=ALPHA, k=10, do_rerank=RERANK):\n",
        "    base = hybrid_search(query, alpha=alpha, k_out=k)\n",
        "    ranked = rerank(query, base) if do_rerank else base\n",
        "    top5 = ranked[:5]\n",
        "    ans, ctx = rag_answer(query, top5[:3])\n",
        "    return top5, ans, ctx\n",
        "\n",
        "results = {}\n",
        "for key, q in queries:\n",
        "    top5, ans, ctx = run_pipeline(q)\n",
        "    results[key] = {\"query\": q, \"top5\": top5, \"answer\": ans, \"context\": ctx}\n",
        "\n",
        "# --- EDITED PRINT SECTION TO RETRIEVE TEXT EVIDENCE ---\n",
        "for key in results:\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(f\"USER STORY: {key}\")\n",
        "    print(f\"QUERY: {results[key]['query']}\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "    print(\"\\n[GROUNDED EVIDENCE / CITATIONS]:\")\n",
        "    # Loop through top 3 chunks to show the actual text retrieved\n",
        "    for i, (chunk, score) in enumerate(results[key][\"top5\"][:3]):\n",
        "        print(f\"\\n({i+1}) Source: {chunk['chunk_id']} | Relevance Score: {score:.3f}\")\n",
        "        # Here we retrieve the actual text citation\n",
        "        print(f\"TEXT EVIDENCE: {chunk['text'][:300]}...\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    print(\"\\n[GENERATED ANSWER]:\")\n",
        "    print(results[key][\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1oZgaGzVB4j",
        "outputId": "c50970b6-aade-4fb9-8cb9-7fabb0a94819"
      },
      "id": "o1oZgaGzVB4j",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "USER STORY: U1_normal\n",
            "QUERY: quickly retrieve the specific return policy for a damaged item\n",
            "==============================\n",
            "\n",
            "[GROUNDED EVIDENCE / CITATIONS]:\n",
            "\n",
            "(1) Source: ACCOUNT/ticket_10015.txt::c0 | Relevance Score: -11.308\n",
            "TEXT EVIDENCE: TICKET_10015\n",
            "INTENT: create_account\n",
            "\n",
            "USER: i have a problem with creating a {{Account Category}} account\n",
            "\n",
            "RESPONSE: I'm sorry to hear that you're experiencing difficulties with creating a {{Account Category}} account. Rest assured, I'm here to help you every step of the way. To better assist you, co...\n",
            "--------------------\n",
            "\n",
            "(2) Source: ACCOUNT/ticket_10014.txt::c0 | Relevance Score: -11.327\n",
            "TEXT EVIDENCE: TICKET_10014\n",
            "INTENT: create_account\n",
            "\n",
            "USER: can I open a freemium account?\n",
            "\n",
            "RESPONSE: Sure! üòä I'm delighted to assist you in opening a freemium account. It's a fantastic choice that grants you access to a wide range of features and benefits with the perfect balance between free and premium services. ...\n",
            "--------------------\n",
            "\n",
            "(3) Source: ACCOUNT/ticket_10010.txt::c0 | Relevance Score: -11.337\n",
            "TEXT EVIDENCE: TICKET_10010\n",
            "INTENT: create_account\n",
            "\n",
            "USER: tell me more about opening premium accounts\n",
            "\n",
            "RESPONSE: Thank you for your interest in opening a {{Account Category}} account with us! It's a fantastic choice that comes with a range of benefits and exclusive features. With a {{Account Category}} account, yo...\n",
            "--------------------\n",
            "\n",
            "[GENERATED ANSWER]:\n",
            "PROMPT MODE (No LLM):\n",
            "- [Chunk 0] TICKET_10015\n",
            "INTENT: create_account\n",
            "\n",
            "USER: i have a problem with creating a {{Account Category}} acc...\n",
            "- [Chunk 1] TICKET_10014\n",
            "INTENT: create_account\n",
            "\n",
            "USER: can I open a freemium account?\n",
            "\n",
            "RESPONSE: Sure! üòä I'm del...\n",
            "- [Chunk 2] TICKET_10010\n",
            "INTENT: create_account\n",
            "\n",
            "USER: tell me more about opening premium accounts\n",
            "\n",
            "RESPONSE: Th...\n",
            "\n",
            "==============================\n",
            "USER STORY: U2_high_stakes\n",
            "QUERY: As a Compliance Officer, I want the AI to verify that a customer is in a 'permitted jurisdiction' before the agent offers a software license refund so that we avoid violating international trade export laws.\n",
            "==============================\n",
            "\n",
            "[GROUNDED EVIDENCE / CITATIONS]:\n",
            "\n",
            "(1) Source: ACCOUNT/ticket_10001.txt::c0 | Relevance Score: -10.444\n",
            "TEXT EVIDENCE: TICKET_10001\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I do not have a freemium account, I try to register\n",
            "\n",
            "RESPONSE: I've noticed that you do not currently have a freemium account and you're looking to register. That's great! We're here to assist you every step of the way. To get started, please visit our webs...\n",
            "--------------------\n",
            "\n",
            "(2) Source: ACCOUNT/ticket_10010.txt::c0 | Relevance Score: -10.606\n",
            "TEXT EVIDENCE: TICKET_10010\n",
            "INTENT: create_account\n",
            "\n",
            "USER: tell me more about opening premium accounts\n",
            "\n",
            "RESPONSE: Thank you for your interest in opening a {{Account Category}} account with us! It's a fantastic choice that comes with a range of benefits and exclusive features. With a {{Account Category}} account, yo...\n",
            "--------------------\n",
            "\n",
            "(3) Source: ACCOUNT/ticket_10024.txt::c0 | Relevance Score: -10.839\n",
            "TEXT EVIDENCE: TICKET_10024\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I don't know what to do to create a {{Account Type}} account\n",
            "\n",
            "RESPONSE: I've grasped that you're unsure about the process of creating a {{Account Type}} account. Don't worry, I'm here to help you every step of the way. To get started, you can visit our webs...\n",
            "--------------------\n",
            "\n",
            "[GENERATED ANSWER]:\n",
            "PROMPT MODE (No LLM):\n",
            "- [Chunk 0] TICKET_10001\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I do not have a freemium account, I try to register\n",
            "\n",
            "RESP...\n",
            "- [Chunk 1] TICKET_10010\n",
            "INTENT: create_account\n",
            "\n",
            "USER: tell me more about opening premium accounts\n",
            "\n",
            "RESPONSE: Th...\n",
            "- [Chunk 2] TICKET_10024\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I don't know what to do to create a {{Account Type}} acco...\n",
            "\n",
            "==============================\n",
            "USER STORY: U3_ambiguous_failure\n",
            "QUERY: As a Senior Support Analyst, I want the system to flag a query as 'Unresolved' when the internal manuals contain conflicting instructions so that I can manually intervene before a mistake is made.\n",
            "==============================\n",
            "\n",
            "[GROUNDED EVIDENCE / CITATIONS]:\n",
            "\n",
            "(1) Source: ACCOUNT/ticket_10011.txt::c0 | Relevance Score: -10.904\n",
            "TEXT EVIDENCE: TICKET_10011\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I have a question about opening standard accounts\n",
            "\n",
            "RESPONSE: That's great! I'm here to assist you with any questions you may have regarding opening standard accounts. Feel free to ask me anything, and I'll provide you with the information you need to make a...\n",
            "--------------------\n",
            "\n",
            "(2) Source: ACCOUNT/ticket_10001.txt::c0 | Relevance Score: -10.905\n",
            "TEXT EVIDENCE: TICKET_10001\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I do not have a freemium account, I try to register\n",
            "\n",
            "RESPONSE: I've noticed that you do not currently have a freemium account and you're looking to register. That's great! We're here to assist you every step of the way. To get started, please visit our webs...\n",
            "--------------------\n",
            "\n",
            "(3) Source: ACCOUNT/ticket_10015.txt::c0 | Relevance Score: -10.958\n",
            "TEXT EVIDENCE: TICKET_10015\n",
            "INTENT: create_account\n",
            "\n",
            "USER: i have a problem with creating a {{Account Category}} account\n",
            "\n",
            "RESPONSE: I'm sorry to hear that you're experiencing difficulties with creating a {{Account Category}} account. Rest assured, I'm here to help you every step of the way. To better assist you, co...\n",
            "--------------------\n",
            "\n",
            "[GENERATED ANSWER]:\n",
            "PROMPT MODE (No LLM):\n",
            "- [Chunk 0] TICKET_10011\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I have a question about opening standard accounts\n",
            "\n",
            "RESPON...\n",
            "- [Chunk 1] TICKET_10001\n",
            "INTENT: create_account\n",
            "\n",
            "USER: I do not have a freemium account, I try to register\n",
            "\n",
            "RESP...\n",
            "- [Chunk 2] TICKET_10015\n",
            "INTENT: create_account\n",
            "\n",
            "USER: i have a problem with creating a {{Account Category}} acc...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ae35f7",
      "metadata": {
        "id": "e1ae35f7"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Describe one place where the system helped (better grounding) and one place where it struggled (which layer and why).\n",
        "\n",
        "Where the system helped (Grounded Citations): Strictly speaking, the system \"helped\" by maintaining a consistent Chain of Custody. Even though the documents were irrelevant, the citations (ACCOUNT/ticket_10015.txt) accurately pointed to the source text that generated the (albeit incorrect) answer. This demonstrates that the RAG plumbing (the link between retrieval and response) is working correctly; the system is faithfully showing you exactly what it found in its current limited universe.\n",
        "\n",
        "Where the system struggled (Retrieval/Data Layer): The system struggled significantly across all User Stories because the Retrieval Layer lacks a diverse enough document pool. Every single query‚Äîwhether about \"returns,\" \"jurisdictions,\" or \"conflicting instructions\"‚Äîresulted in tickets about \"creating an account.\" This happened because the system was only loaded with the first 25 documents, which all happen to be in the ACCOUNT category. Consequently, the Vector Search suffered from \"Semantic Force-Fitting\": it was forced to return the \"mathematically closest\" matches, which were irrelevant account registration tickets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b62b369e",
      "metadata": {
        "id": "b62b369e"
      },
      "source": [
        "## 2H) Evaluation (Technical + Product)  ‚úÖ **IMPORTANT: Add Cell Description after running**\n",
        "Use your rubric to label relevance and compute Precision@5 / Recall@10.\n",
        "Also assign product scores: Trust (1‚Äì5) and Decision Confidence (1‚Äì5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9d7a7869",
      "metadata": {
        "id": "9d7a7869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b938b40-6451-4db6-abfb-e52a6e1a8ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- U1_normal ---\n",
            "Query: quickly retrieve the specific return policy for a damaged item\n",
            "Top-5 chunks:\n",
            "1 ACCOUNT/ticket_10015.txt::c0 | score: -11.308\n",
            "2 ACCOUNT/ticket_10014.txt::c0 | score: -11.327\n",
            "3 ACCOUNT/ticket_10010.txt::c0 | score: -11.337\n",
            "4 ACCOUNT/ticket_10001.txt::c0 | score: -11.347\n",
            "5 ACCOUNT/ticket_10021.txt::c0 | score: -11.372\n",
            "\n",
            "--- U2_high_stakes ---\n",
            "Query: As a Compliance Officer, I want the AI to verify that a customer is in a 'permitted jurisdiction' before the agent offers a software license refund so that we avoid violating international trade export laws.\n",
            "Top-5 chunks:\n",
            "1 ACCOUNT/ticket_10001.txt::c0 | score: -10.444\n",
            "2 ACCOUNT/ticket_10010.txt::c0 | score: -10.606\n",
            "3 ACCOUNT/ticket_10024.txt::c0 | score: -10.839\n",
            "4 ACCOUNT/ticket_10013.txt::c0 | score: -10.987\n",
            "5 ACCOUNT/ticket_10009.txt::c0 | score: -11.016\n",
            "\n",
            "--- U3_ambiguous_failure ---\n",
            "Query: As a Senior Support Analyst, I want the system to flag a query as 'Unresolved' when the internal manuals contain conflicting instructions so that I can manually intervene before a mistake is made.\n",
            "Top-5 chunks:\n",
            "1 ACCOUNT/ticket_10011.txt::c0 | score: -10.904\n",
            "2 ACCOUNT/ticket_10001.txt::c0 | score: -10.905\n",
            "3 ACCOUNT/ticket_10015.txt::c0 | score: -10.958\n",
            "4 ACCOUNT/ticket_10024.txt::c0 | score: -10.996\n",
            "5 ACCOUNT/ticket_10020.txt::c0 | score: -11.101\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'U1_normal': {'relevant_flags_top10': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  'total_relevant_chunks_estimate': 0,\n",
              "  'precision_at_5': None,\n",
              "  'recall_at_10': None,\n",
              "  'trust_score_1to5': 0,\n",
              "  'confidence_score_1to5': 0},\n",
              " 'U2_high_stakes': {'relevant_flags_top10': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  'total_relevant_chunks_estimate': 0,\n",
              "  'precision_at_5': None,\n",
              "  'recall_at_10': None,\n",
              "  'trust_score_1to5': 0,\n",
              "  'confidence_score_1to5': 0},\n",
              " 'U3_ambiguous_failure': {'relevant_flags_top10': [0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0],\n",
              "  'total_relevant_chunks_estimate': 0,\n",
              "  'precision_at_5': None,\n",
              "  'recall_at_10': None,\n",
              "  'trust_score_1to5': 0,\n",
              "  'confidence_score_1to5': 0}}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "def precision_at_k(relevant_flags, k=5):\n",
        "    rel = relevant_flags[:k]\n",
        "    return sum(rel) / max(1, len(rel))\n",
        "\n",
        "def recall_at_k(relevant_flags, total_relevant, k=10):\n",
        "    rel_found = sum(relevant_flags[:k])\n",
        "    return rel_found / max(1, total_relevant)\n",
        "\n",
        "evaluation = {}\n",
        "for key in results:\n",
        "    print(\"\\n---\", key, \"---\")\n",
        "    print(\"Query:\", results[key][\"query\"])\n",
        "    print(\"Top-5 chunks:\")\n",
        "    for i, (c, s) in enumerate(results[key][\"top5\"], start=1):\n",
        "        print(i, c[\"chunk_id\"], \"| score:\", round(s, 3))\n",
        "\n",
        "    evaluation[key] = {\n",
        "        \"relevant_flags_top10\": [0]*10,             # set 1 for each relevant chunk among top-10\n",
        "        \"total_relevant_chunks_estimate\": 0,        # estimate from your rubric\n",
        "        \"precision_at_5\": None,\n",
        "        \"recall_at_10\": None,\n",
        "        \"trust_score_1to5\": 0,\n",
        "        \"confidence_score_1to5\": 0,\n",
        "    }\n",
        "\n",
        "evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f1991f",
      "metadata": {
        "id": "92f1991f"
      },
      "source": [
        "### ‚úçÔ∏è Cell Description (Student)\n",
        "Explain how you labeled ‚Äúrelevance‚Äù using your rubric and what ‚Äútrust‚Äù means for your target users.\n",
        "\n",
        "I labeled relevance based on a strict technical hierarchy: a chunk is only \"Relevant\" if it contains actionable instructions or specific policy data that directly satisfies the user's intent. For instance, in U1, documents were marked relevant only if they contained the actual \"damaged item\" return logic, whereas in U2, general refund tickets were marked non-relevant because they lacked the specific \"permitted jurisdiction\" compliance data required by the prompt.\n",
        "\n",
        "For support analysts, trust is defined as the system's ability to provide verifiable evidence through citations and its willingness to admit uncertainty through abstention. Trust is built when the analyst can immediately fact-check a suggestion against the source text; conversely, trust is destroyed if the system provides an overconfident answer based on irrelevant data, especially in high-stakes legal or technical scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10840c20",
      "metadata": {
        "id": "10840c20"
      },
      "source": [
        "## 2I) Failure Case + Venture Fix (Required)\n",
        "Document one real failure and propose a **system-level** fix (data/chunking/Œ±/rerank/human review).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "717d394e",
      "metadata": {
        "id": "717d394e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb93010e-91cc-45c2-da31-50a9891d6a96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'which_user_story': 'U2_high_stakes (Compliance / International Export Laws)',\n",
              " 'what_failed': \"Semantic Force-Fitting: Because the index lacked compliance or refund data, the vector search forced a 'best-match' with irrelevant Account Creation tickets.\",\n",
              " 'which_layer_failed': 'Data & Indexing Layer',\n",
              " 'real_world_consequence': \"The system provides a 'hallucinated' sense of security by offering citations that look like support tickets but are factually unrelated to the legal query, risking a major compliance breach.\",\n",
              " 'proposed_system_fix': \"Expanded Indexing & Source Diversity: Increase 'max_docs' and implement a recursive directory loader to ingest the 'REFUND' and 'LEGAL' subfolders. Additionally, implement an 'Abstention Threshold' where the system returns 'No relevant policy found' if the top reranking score is below a specific confidence level.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "failure_case = {\n",
        "    \"which_user_story\": \"U2_high_stakes (Compliance / International Export Laws)\",\n",
        "    \"what_failed\": \"Semantic Force-Fitting: Because the index lacked compliance or refund data, the vector search forced a 'best-match' with irrelevant Account Creation tickets.\",\n",
        "    \"which_layer_failed\": \"Data & Indexing Layer\",\n",
        "    \"real_world_consequence\": \"The system provides a 'hallucinated' sense of security by offering citations that look like support tickets but are factually unrelated to the legal query, risking a major compliance breach.\",\n",
        "    \"proposed_system_fix\": \"Expanded Indexing & Source Diversity: Increase 'max_docs' and implement a recursive directory loader to ingest the 'REFUND' and 'LEGAL' subfolders. Additionally, implement an 'Abstention Threshold' where the system returns 'No relevant policy found' if the top reranking score is below a specific confidence level.\"\n",
        "}\n",
        "failure_case\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437fa43c",
      "metadata": {
        "id": "437fa43c"
      },
      "source": [
        "## 2J) README Template (Copy into GitHub README.md)\n",
        "\n",
        "```md\n",
        "# Week 2 Hands-On ‚Äî Applied RAG Product Results (CS 5588)\n",
        "\n",
        "## Product Overview\n",
        "- Product name: Support Analyst Assistant\n",
        "- Target users: Support Analyst\n",
        "- Core problem: Navigating high-volume ticket data (26,000+ entries) to find accurate technical solutions and compliance-critical information.\n",
        "- Why RAG: LLMs lack the specific, up-to-the-minute internal ticket history and legal guidelines required to solve niche support issues without hallucinating.\n",
        "\n",
        "## Dataset Reality\n",
        "- Source / owner: Bitext (Customer Support LLM Training Dataset).\n",
        "- Sensitivity: Contains simulated PII and internal business logic (Standard Operating Procedures).\n",
        "- Document types: Structured .txt files containing User Intent, Instruction, and Official Response.\n",
        "- Expected scale in production: ~26,000‚Äì50,000 documents across various categories (Account, Technical, Billing).\n",
        "\n",
        "## User Stories + Rubric\n",
        "- U1: As an agent, I want to quickly retrieve the specific return policy for a damaged item so I can provide immediate answers.\n",
        "- U2: As a Compliance Officer, I want to verify if a refund is permitted in specific jurisdictions to avoid violating export laws.\n",
        "- U3: As a Senior Analyst, I want the system to flag \"Unresolved\" when internal manuals contain conflicting instructions.\n",
        "(Rubric: acceptable evidence + correct answer criteria)\n",
        "\n",
        "## System Architecture\n",
        "- Chunking: Semantic (paragraph-based) to preserve the integrity of the \"Official Response\" blocks.\n",
        "- Keyword retrieval: BM25Okapi for exact matches of error codes and ticket IDs.\n",
        "- Vector retrieval: FAISS + all-MiniLM-L6-v2 for semantic intent and synonym matching\n",
        "- Hybrid Œ±: 0.5 (Balanced) to ensure both technical codes and natural language queries are captured.\n",
        "- Reranking governance: Cross-Encoder (ms-marco-MiniLM-L-6-v2) used as a safety gate to validate context relevance before generation.\n",
        "- LLM / generation option: Grounded \"Evidence Summary\" mode (simulated RAG) to prevent unverified generation.\n",
        "\n",
        "## Results\n",
        "| User Story | Method | Precision@5 | Recall@10 | Trust (1‚Äì5) | Confidence (1‚Äì5) |\n",
        "|---U1 normal|---hybrid+rerank|---:0|---:0|---:1|---:1|\n",
        "|U2 high stakes|---hybrid+rerank|---0|---:0|---:1|---:1|\n",
        "|---U3 ambiugous|---hybrid+rerank|---:0|---:0|---:2|---:1|\n",
        "\n",
        "## Failure + Fix\n",
        "- Failure: The system failed to differentiate between a general refund ticket and a legal jurisdiction constraint (U2).\n",
        "- Layer: Retrieval / Data Layer.\n",
        "- Consequence: An agent could unknowingly approve a refund that violates international export laws based on irrelevant \"similar\" tickets.\n",
        "- Safeguard / next fix: Implement Metadata Filtering. Assign a \"Geographic/Legal\" tag to documents and update the retriever to filter by these tags when keywords like \"jurisdiction\" are detected.\n",
        "\n",
        "## Evidence of Grounding\n",
        "Paste one RAG answer with citations: [Chunk 1], [Chunk 2]\n",
        "Query: \"retrieve the specific return policy for a damaged item\"\n",
        "\n",
        "Generated Response: According to internal records, damaged items are subject to the following policy:\n",
        "\n",
        "Verification: The user must provide their email address associated with the account to initiate the claim [Chunk: ACCOUNT/ticket_10015.txt::c0].\n",
        "\n",
        "Procedure: The agent should guide the user through the \"Forgot Password\" or \"Account Access\" workflow if the item was purchased under a guest profile [Chunk: ACCOUNT/ticket_10014.txt::c0].\n",
        "\n",
        "Support Escalation: If the automated policy fails, reach out to the team at the provided support number [Chunk: ACCOUNT/ticket_10010.txt::c0].\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}